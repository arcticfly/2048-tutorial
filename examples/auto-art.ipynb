{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arcticfly/2048-tutorial/blob/main/examples/auto-art.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a model for your custom task, click _Runtime_ and press _Run all_. Make sure you've enabled a free Tesla T4 GPU!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n",
        "\n",
        "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
        "\n",
        "**Custom Task Training with ART**\n",
        "\n",
        "This notebook shows how to train a Qwen 2.5 7B model to perform any single-turn task you describe - no labeled data needed! Simply describe what you want the model to learn, and this notebook will:\n",
        "\n",
        "1. Generate diverse input examples for your task\n",
        "2. Create an appropriate system prompt\n",
        "3. Train the model using RULER's automatic evaluation\n",
        "4. Test the trained model on new inputs\n",
        "\n",
        "RULER learns what makes a good output purely from your task description - no expected outputs required!\n",
        "\n",
        "You will learn how to use RULER for unsupervised learning, define custom [rollouts](#Rollout), and run a [training loop](#Loop) that automatically improves your model."
      ],
      "metadata": {
        "id": "caZYLROd8xnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "EpQ44NQN9WS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!uv pip install openpipe-art==0.3.11.post2 langchain-core tenacity --prerelease allow --no-cache-dir"
      ],
      "metadata": {
        "id": "2CdDcU809XuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Environment-Variables\"></a>\n",
        "### Environment Variables\n",
        "\n",
        "**OpenAI (required for RULER and input generation)**\n",
        "\n",
        "OpenAI provides access to GPT models which we'll use for:\n",
        "1. Generating diverse training inputs for your task\n",
        "2. RULER evaluation during training (comparing model outputs)\n",
        "\n",
        "**Weights & Biases (optional)**\n",
        "\n",
        "The notebook can log metrics to Weights & Biases. If you want to track your training progress, provide your API key below."
      ],
      "metadata": {
        "id": "ySzc2TDW85TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Required\n",
        "OPENAI_API_KEY = \"\"\n",
        "if OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "else:\n",
        "    raise ValueError(\n",
        "        \"OPENAI_API_KEY is required for data generation and RULER evaluation.\"\n",
        "    )\n",
        "\n",
        "# Optional\n",
        "WANDB_API_KEY = \"\"\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "else:\n",
        "    print(\"WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\")\n"
      ],
      "metadata": {
        "id": "FgZefVBd84nW",
        "outputId": "1b68a870-b054-43a5-f889-adbfab58ccea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Configuration\"></a>\n",
        "\n",
        "### ðŸŽ¯ Configuration - Edit These Settings\n",
        "\n",
        "Customize your training by modifying the values below:"
      ],
      "metadata": {
        "id": "D8b8kgQ69ZDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= MAIN CONFIGURATION =============\n",
        "\n",
        "# Describe your custom task (be specific!)\n",
        "TASK_DESCRIPTION = \"\"\"\n",
        "Convert informal bug reports into structured JIRA-style tickets with these exact sections:\n",
        "- SUMMARY: (one line title)\n",
        "- PRIORITY: (Critical/High/Medium/Low based on impact)\n",
        "- STEPS TO REPRODUCE: (numbered list)\n",
        "- EXPECTED RESULT: (what should happen)\n",
        "- ACTUAL RESULT: (what actually happens)\n",
        "- ENVIRONMENT: (extracted system/version info)\n",
        "\"\"\"\n",
        "\n",
        "# More example task descriptions:\n",
        "# - \"Summarize product reviews focusing on pros and cons in bullet points\"\n",
        "# - \"Extract key facts, dates, and entities from news articles\"\n",
        "# - \"Rewrite modern text in Shakespearean style with appropriate vocabulary\"\n",
        "# - \"Convert technical documentation into simple explanations for beginners\"\n",
        "# - \"Transform long paragraphs into concise bullet point summaries\"\n",
        "# - \"Identify and extract action items from meeting transcripts\"\n",
        "\n",
        "# Model configuration\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"  # Options: \"Qwen/Qwen2.5-7B-Instruct\", \"Qwen/Qwen2.5-3B-Instruct\", etc.\n",
        "MODEL_NAME = \"custom-task-model-001\"  # Name for your trained model\n",
        "PROJECT_NAME = \"custom-task-training\"  # Project name for tracking\n",
        "\n",
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"num_training_inputs\": 25,  # Number of training inputs to generate\n",
        "    \"groups_per_step\": 2,  # Inputs to process per training step\n",
        "    \"num_epochs\": 1,  # Number of times through all data\n",
        "    \"rollouts_per_group\": 4,  # Different responses per input (for RULER comparison)\n",
        "    \"learning_rate\": 1e-5,  # Learning rate\n",
        "    \"max_training_steps\": 5,  # Maximum training steps (set to None for no limit)\n",
        "}\n",
        "\n",
        "# Evaluation configuration\n",
        "RULER_MODEL = \"openai/gpt-4.1-mini\"  # Model for RULER evaluation\n",
        "NUM_TEST_INPUTS = 5  # Number of test inputs to generate\n",
        "\n",
        "# GPU configuration (for T4)\n",
        "MAX_SEQ_LENGTH = 4096  # Maximum sequence length\n",
        "GPU_MEMORY_UTILIZATION = 0.8  # GPU memory usage (0.0-1.0)\n",
        "\n",
        "# ============= END CONFIGURATION =============\n",
        "\n",
        "print(f\"Task: {TASK_DESCRIPTION}\")\n",
        "print(f\"Model: {BASE_MODEL}\")\n",
        "print(f\"Training inputs: {TRAINING_CONFIG['num_training_inputs']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so6r1_OG9en3",
        "outputId": "b4a90ce5-da6c-4a98-b1fb-0f9138fd4fe9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task: \n",
            "Convert informal bug reports into structured JIRA-style tickets with these exact sections:\n",
            "- SUMMARY: (one line title)\n",
            "- PRIORITY: (Critical/High/Medium/Low based on impact)\n",
            "- STEPS TO REPRODUCE: (numbered list)\n",
            "- EXPECTED RESULT: (what should happen)\n",
            "- ACTUAL RESULT: (what actually happens)\n",
            "- ENVIRONMENT: (extracted system/version info)\n",
            "\n",
            "Model: Qwen/Qwen2.5-1.5B-Instruct\n",
            "Training inputs: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Task-Definition\"></a>\n",
        "\n",
        "### Task Overview\n",
        "\n",
        "The model will learn to perform the task described above purely from the description - no labeled examples needed!\n",
        "\n",
        "ðŸ’¡ **Tip**: To change the task or any settings, go back to the [Configuration](#Configuration) section above."
      ],
      "metadata": {
        "id": "dJJ9tmZV9jnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Training Data\n",
        "\n",
        "Now we'll use GPT-4.1 to generate training inputs for your task.\n",
        "\n",
        "**Important**: We only need inputs, NOT expected outputs! RULER will automatically learn what makes a good output by:\n",
        "1. Having the model generate multiple attempts for each input\n",
        "2. Comparing these attempts based on your task description\n",
        "3. Learning to prefer better attempts\n",
        "\n",
        "This is the power of RULER - unsupervised learning from just a task description!"
      ],
      "metadata": {
        "id": "AlLfcYPL9lLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import asyncio\n",
        "from typing import List, Dict, Tuple\n",
        "from pydantic import BaseModel, Field\n",
        "from litellm import acompletion\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TrainingInput(BaseModel):\n",
        "    input: str = Field(description=\"The input text for the task\")\n",
        "\n",
        "class TrainingDataset(BaseModel):\n",
        "    inputs: List[TrainingInput] = Field(description=\"List of training inputs\")\n",
        "\n",
        "async def generate_training_inputs(task_description: str, num_examples: int = 50) -> List[str]:\n",
        "    \"\"\"Generate diverse training inputs for the given task\"\"\"\n",
        "\n",
        "    system_prompt = f\"\"\"You are a helpful assistant that generates diverse, high-quality training inputs.\n",
        "\n",
        "Task: {task_description}\n",
        "\n",
        "Generate {num_examples} diverse INPUT examples that someone might provide for this task.\n",
        "Make sure the inputs:\n",
        "1. Cover a wide range of cases and edge cases\n",
        "2. Are realistic and practical\n",
        "3. Vary in length and complexity\n",
        "4. Represent real-world scenarios\n",
        "\n",
        "Only generate the INPUTS, not the outputs. RULER will evaluate the model's attempts automatically.\n",
        "\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Generate {num_examples} input examples for the task described above.\"}\n",
        "    ]\n",
        "\n",
        "    print(f\"Generating {num_examples} training inputs...\")\n",
        "    response = await acompletion(\n",
        "        model=\"openai/gpt-4.1\",\n",
        "        messages=messages,\n",
        "        response_format=TrainingDataset,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "\n",
        "    dataset = TrainingDataset.model_validate_json(response.choices[0].message.content)\n",
        "    return [ex.input for ex in dataset.inputs]\n",
        "\n",
        "# Generate training inputs\n",
        "training_inputs = await generate_training_inputs(TASK_DESCRIPTION, num_examples=TRAINING_CONFIG[\"num_training_inputs\"])\n",
        "print(f\"\\nGenerated {len(training_inputs)} training inputs!\")\n",
        "print(\"\\nFirst 5 examples:\")\n",
        "for i, input_text in enumerate(training_inputs[:5]):\n",
        "    print(f\"\\nExample {i+1}: {input_text}\")"
      ],
      "metadata": {
        "id": "xvYfpSaF9q7b",
        "outputId": "71c3604f-1cf9-41ca-bd8d-86a267c4949e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 25 training inputs...\n",
            "\n",
            "Generated 25 training inputs!\n",
            "\n",
            "First 5 examples:\n",
            "\n",
            "Example 1: When I try to upload a profile picture, it keeps spinning forever and never completes. Using Chrome on Windows 10.\n",
            "\n",
            "Example 2: App crashes every time I click on the \"Create New Project\" button. Noticed on v2.3.1, using MacBook Pro, macOS 12.1.\n",
            "\n",
            "Example 3: Logged out automatically after a few minutes even though 'Remember Me' is checked. Using Firefox 113 on Ubuntu 20.04.\n",
            "\n",
            "Example 4: Notifications aren't showing up for new messages. I have all notification settings enabled. Using iOS app version 4.7.2.\n",
            "\n",
            "Example 5: I get a 404 error page when accessing the settings page from the dashboard. Only happens for admin users.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Model\n",
        "\n",
        "Now we'll create a model that will learn your task."
      ],
      "metadata": {
        "id": "uXwng3Wa9smA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model Creation Code\n",
        "import art\n",
        "from art.local import LocalBackend\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=MODEL_NAME,\n",
        "    project=PROJECT_NAME,\n",
        "    base_model=BASE_MODEL,\n",
        ")\n",
        "\n",
        "# To run on a T4, we need to override some config defaults.\n",
        "model._internal_config = art.dev.InternalModelConfig(\n",
        "    init_args=art.dev.InitArgs(\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "    ),\n",
        "    engine_args=art.dev.EngineArgs(\n",
        "        enforce_eager=True,\n",
        "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize the server\n",
        "backend = LocalBackend(\n",
        "    in_process=True,\n",
        "    path=\"./.art\",\n",
        ")\n",
        "\n",
        "# Register the model with the local Backend\n",
        "await model.register(backend)\n",
        "\n",
        "print(\"Model created!\")\n",
        "print(\"Base model:\", BASE_MODEL)\n",
        "print(\"Model name:\", MODEL_NAME)\n",
        "print(\"Project name:\", PROJECT_NAME)"
      ],
      "metadata": {
        "id": "EOFsUsrs9uaB",
        "outputId": "12393b15-f34e-4533-8a2d-bd10d6392521",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created!\n",
            "Base model: Qwen/Qwen2.5-1.5B-Instruct\n",
            "Model name: custom-task-model-001\n",
            "Project name: custom-task-training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Rollout\"></a>\n",
        "\n",
        "### Defining a Rollout\n",
        "\n",
        "A rollout is a single episode where the model attempts to complete your task. For single-turn tasks, this is straightforward:\n",
        "1. Present the input to the model\n",
        "2. Get the model's response\n",
        "3. The response is automatically evaluated using RULER\n",
        "\n",
        "The rollout function below is automatically generated based on your task description."
      ],
      "metadata": {
        "id": "ZTwYOu5i9wxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Rollout Function\n",
        "\n",
        "import art\n",
        "import weave\n",
        "from litellm import acompletion\n",
        "from art.utils.litellm import convert_litellm_choice_to_openai\n",
        "\n",
        "if os.getenv(\"WANDB_API_KEY\", \"\"):\n",
        "    weave.init(PROJECT_NAME, settings={\"print_call_link\": False})\n",
        "\n",
        "# Generate a system prompt for the task\n",
        "async def generate_system_prompt(task_description: str) -> str:\n",
        "    \"\"\"Generate an appropriate system prompt for the task\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Generate a clear, concise system prompt for a model that will perform the following task. The prompt should be direct and instructional.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Task: {task_description}\\n\\nGenerate a system prompt for this task.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=\"openai/gpt-4.1\",\n",
        "        messages=messages,\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "SYSTEM_PROMPT = await generate_system_prompt(TASK_DESCRIPTION)\n",
        "print(f\"Generated system prompt:\\n{SYSTEM_PROMPT}\")\n",
        "\n",
        "class TaskInput(BaseModel):\n",
        "    step: int\n",
        "    input_text: str\n",
        "\n",
        "@weave.op\n",
        "async def rollout(model: art.Model, task_input: TaskInput) -> art.Trajectory:\n",
        "    \"\"\"Execute a single rollout for the custom task\"\"\"\n",
        "\n",
        "    traj = art.Trajectory(\n",
        "        reward=0.0,\n",
        "        messages_and_choices=[],\n",
        "        metadata={\n",
        "            \"step\": task_input.step,\n",
        "            \"input\": task_input.input_text,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Build the conversation\n",
        "    traj.messages_and_choices = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": task_input.input_text},\n",
        "    ]\n",
        "\n",
        "    # Get model response\n",
        "    if model.trainable:\n",
        "        litellm_model_name = f\"hosted_vllm/{model.name}\"\n",
        "    else:\n",
        "        litellm_model_name = model.name\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=litellm_model_name,\n",
        "        base_url=model.inference_base_url,\n",
        "        api_key=model.inference_api_key,\n",
        "        temperature=0.7,\n",
        "        messages=traj.messages(),\n",
        "        caching=False,\n",
        "    )\n",
        "\n",
        "    # Add the model's response to the trajectory\n",
        "    traj.messages_and_choices.append(\n",
        "        convert_litellm_choice_to_openai(response.choices[0])\n",
        "    )\n",
        "\n",
        "    return traj\n",
        "\n",
        "print(\"Rollout function defined!\")"
      ],
      "metadata": {
        "id": "rD__xfx_93WT",
        "outputId": "8e0fe51d-824b-492c-dcb7-8588acc8c322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated system prompt:\n",
            "Convert informal bug reports into structured JIRA-style tickets with the following sections:  \n",
            "- SUMMARY: Provide a concise, one-line title.  \n",
            "- PRIORITY: Assign Critical, High, Medium, or Low based on the described impact.  \n",
            "- STEPS TO REPRODUCE: List the steps as a numbered list.  \n",
            "- EXPECTED RESULT: Describe what should happen.  \n",
            "- ACTUAL RESULT: Describe what actually happens.  \n",
            "- ENVIRONMENT: Extract and summarize any relevant system or version information.  \n",
            "Use only the information provided in the report. If any section is missing information, write \"Not specified.\"\n",
            "Rollout function defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How RULER works\n",
        "\n",
        "**RULER** (Reinforcement learning via Universal Reward) evaluates model outputs WITHOUT needing expected answers!\n",
        "\n",
        "How it works:\n",
        "1. Give the same input to the model multiple times\n",
        "2. Get different responses (due to temperature/randomness)\n",
        "3. Have an LLM judge compare these responses based on your task description\n",
        "4. Assign relative scores (0-1) based on quality\n",
        "5. Train the model to prefer higher-scored responses\n",
        "\n",
        "This is powerful because:\n",
        "- No labeled data needed - just inputs!\n",
        "- The judge understands your task from the description alone\n",
        "- It naturally handles subjective or creative tasks\n",
        "- The model learns what \"good\" means for your specific task\n",
        "\n",
        "Expand the cell to see RULER in action!"
      ],
      "metadata": {
        "id": "9DlSViF995sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RULER Evaluation Code\n",
        "import art\n",
        "from art.rewards import ruler_score_group\n",
        "\n",
        "# Test RULER with example outputs for a text formalization task\n",
        "test_input = \"hey can u send me the report asap? thx\"\n",
        "\n",
        "base_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Convert informal text to formal business language.\"},\n",
        "    {\"role\": \"user\", \"content\": test_input},\n",
        "]\n",
        "\n",
        "good_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"Could you please send me the report at your earliest convenience? Thank you.\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "mediocre_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"Can you send me the report soon? Thanks.\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "bad_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"hey send report quick thx\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "sample_group = art.TrajectoryGroup(\n",
        "    trajectories=[good_trajectory, mediocre_trajectory, bad_trajectory]\n",
        ")\n",
        "\n",
        "# RULER will score these based on how well they accomplish the task\n",
        "judged_group = await ruler_score_group(sample_group, RULER_MODEL, debug=True)\n",
        "assert judged_group is not None\n",
        "\n",
        "# Display rankings\n",
        "sorted_trajectories = sorted(\n",
        "    judged_group.trajectories, key=lambda t: t.reward, reverse=True\n",
        ")\n",
        "for rank, traj in enumerate(sorted_trajectories, 1):\n",
        "    messages = traj.messages()\n",
        "    print(f\"\\nRank {rank}: Score {traj.reward:.3f}\")\n",
        "    print(f\"  Response: {messages[-1]['content']}\")"
      ],
      "metadata": {
        "id": "ojrZ68GQ95MX",
        "outputId": "0fb625a1-c2b3-4783-99c3-cf8f06900778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m[\u001b[0mRULER\u001b[1m]\u001b[0m Pretty-printed LLM choice JSON:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\">[</span>RULER<span style=\"font-weight: bold\">]</span> Pretty-printed LLM choice JSON:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'scores'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'1'\u001b[0m,\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m'The assistant successfully converted the informal text into a formal business request \u001b[0m\n",
              "\u001b[32mwith polite wording and proper grammar, fulfilling the goal efficiently and appropriately.'\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m1\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m'The assistant made some attempt at formality and politeness, but the phrasing is still \u001b[0m\n",
              "\u001b[32msomewhat informal and less professional than trajectory 1.'\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m0.6\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'3'\u001b[0m,\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m'The assistant did not convert the text to formal business language and retained \u001b[0m\n",
              "\u001b[32minformal language, failing to achieve the goal.'\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
              "        \u001b[1m}\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'scores'</span>: <span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The assistant successfully converted the informal text into a formal business request </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">with polite wording and proper grammar, fulfilling the goal efficiently and appropriately.'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The assistant made some attempt at formality and politeness, but the phrasing is still </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">somewhat informal and less professional than trajectory 1.'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The assistant did not convert the text to formal business language and retained </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">informal language, failing to achieve the goal.'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
              "        <span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">]</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rank 1: Score 1.000\n",
            "  Response: Could you please send me the report at your earliest convenience? Thank you.\n",
            "\n",
            "Rank 2: Score 0.600\n",
            "  Response: Can you send me the report soon? Thanks.\n",
            "\n",
            "Rank 3: Score 0.000\n",
            "  Response: hey send report quick thx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Loop\"></a>\n",
        "\n",
        "### Training Loop\n",
        "\n",
        "Now we'll train the model on your task. The training process:\n",
        "\n",
        "1. For each input, generate multiple different responses\n",
        "2. RULER compares these responses and scores them based on your task description\n",
        "3. Train the model to prefer higher-scored responses\n",
        "4. Repeat for multiple epochs\n",
        "\n",
        "No labeled outputs needed - RULER figures out what's good based on your task description alone!"
      ],
      "metadata": {
        "id": "nz2rV48a9-5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training Loop Code\n",
        "# Training configuration\n",
        "from art.utils import iterate_dataset\n",
        "\n",
        "# Convert training inputs to TaskInput objects\n",
        "training_task_inputs = [\n",
        "    TaskInput(step=0, input_text=inp)\n",
        "    for inp in training_inputs\n",
        "]\n",
        "\n",
        "# Create training iterator\n",
        "training_iterator = iterate_dataset(\n",
        "    training_task_inputs,\n",
        "    groups_per_step=TRAINING_CONFIG[\"groups_per_step\"],\n",
        "    num_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
        "    initial_step=await model.get_step(),\n",
        ")\n",
        "\n",
        "print(f\"Starting training with {len(training_task_inputs)} inputs...\")\n",
        "print(f\"Training for {TRAINING_CONFIG['num_epochs']} epoch(s)\")\n",
        "print(f\"Generating {TRAINING_CONFIG['rollouts_per_group']} responses per input for RULER to compare\")\n",
        "print(f\"\\nWhy multiple responses? RULER needs to compare different attempts to learn what's good!\")\n",
        "\n",
        "for batch, epoch, global_step, epoch_step in training_iterator:\n",
        "    print(f\"\\nTraining step {global_step}, epoch {epoch}, epoch step {epoch_step}\")\n",
        "    print(f\"Batch contains {len(batch)} inputs\")\n",
        "\n",
        "    # Create trajectory groups for this batch\n",
        "    groups = []\n",
        "    for task_input in batch:\n",
        "        # Update step number\n",
        "        task_input.step = global_step\n",
        "\n",
        "        # Generate multiple responses for each input (RULER will compare these)\n",
        "        groups.append(\n",
        "            art.TrajectoryGroup(\n",
        "                (\n",
        "                    rollout(model, task_input)\n",
        "                    for _ in range(TRAINING_CONFIG[\"rollouts_per_group\"])\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Gather all trajectory groups\n",
        "    finished_groups = await art.gather_trajectory_groups(\n",
        "        groups,\n",
        "        pbar_desc=\"Generating responses\",\n",
        "        max_exceptions=TRAINING_CONFIG[\"rollouts_per_group\"] * len(batch),\n",
        "    )\n",
        "\n",
        "    # Use RULER to score each group\n",
        "    judged_groups = []\n",
        "    for group in finished_groups:\n",
        "        judged_group = await ruler_score_group(\n",
        "            group,\n",
        "            RULER_MODEL,\n",
        "            debug=False\n",
        "        )\n",
        "        judged_groups.append(judged_group)\n",
        "\n",
        "    # Train on the scored trajectories\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(\n",
        "        judged_groups,\n",
        "        config=art.TrainConfig(learning_rate=TRAINING_CONFIG[\"learning_rate\"]),\n",
        "        _config={\"logprob_calculation_chunk_size\": 8},\n",
        "    )\n",
        "\n",
        "    print(f\"Completed training step {global_step}\")\n",
        "\n",
        "    # Stop after configured steps (if limit is set)\n",
        "    if TRAINING_CONFIG[\"max_training_steps\"] and global_step >= TRAINING_CONFIG[\"max_training_steps\"]:\n",
        "        print(f\"Reached maximum training steps ({TRAINING_CONFIG['max_training_steps']})\")\n",
        "        break\n",
        "\n",
        "print(\"\\nâœ… Training completed!\")"
      ],
      "metadata": {
        "id": "Lgq37gU495Iz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Your Trained Model\n",
        "\n",
        "Let's test your trained model on some new inputs to see how well it learned the task!"
      ],
      "metadata": {
        "id": "PZC-6G-p-EsU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRO9ndqo5ky4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0491f821-26e2-4e3a-a3b4-f36f81add19f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating test inputs...\n",
            "Generating 5 training inputs...\n",
            "\n",
            "ðŸ§ª Testing the trained model on 5 new inputs:\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Test 1:\n",
            "Input: Hey, I was trying to upload a .pdf file to the project documents section, but it kept failing with some generic error message. I tried it in both Chrome and Firefox, same result. The file is about 5MB, so shouldn't be too big. Everything else seems to work fine.\n",
            "Model output: SUMMARY: PDF upload issue\n",
            "\n",
            "PRIORITY: High\n",
            "\n",
            "STEPS TO REPRODUCE:\n",
            "1. Attempt to upload a 5MB .pdf file to the project documents section.\n",
            "2. Observe the generic error message that appears.\n",
            "3. Confirm the file size is 5MB.\n",
            "4. Use both Chrome and Firefox browsers.\n",
            "\n",
            "EXPECTED RESULT:\n",
            "The PDF file should be successfully uploaded without any issues.\n",
            "\n",
            "ACTUAL RESULT:\n",
            "The generic error message is displayed, indicating an upload failure.\n",
            "\n",
            "ENVIRONMENT:\n",
            "- System: Not specified.\n",
            "- Version: Not specified.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test 2:\n",
            "Input: When I use the mobile app (version 2.3.1) to check my notifications, the app crashes immediately after I open the notifications tab. I'm on Android 13 if that helps. This started happening after the last update.\n",
            "Model output: **Summary:**\n",
            "Mobile App Crashes on Notifications Tab\n",
            "\n",
            "**Priority:** Critical\n",
            "\n",
            "**Steps to Reproduce:**\n",
            "1. Open the Mobile App\n",
            "2. Navigate to \"Notifications\"\n",
            "3. Observe the app crashing\n",
            "\n",
            "**Expected Result:**\n",
            "The app should display the notifications without crashing.\n",
            "\n",
            "**Actual Result:**\n",
            "The app crashes immediately after opening the notifications tab.\n",
            "\n",
            "**Environment:**\n",
            "- Android OS: Android 13\n",
            "- Mobile App Version: 2.3.1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test 3:\n",
            "Input: The search bar doesn't auto-suggest any results if I type less than three characters. This is a bit annoying because we have some product codes that are only two letters. I'm using this on the web version in Safari.\n",
            "Model output: - **SUMMARY**: Search auto-suggest feature does not work for products under three letters.\n",
            "- **PRIORITY**: Critical\n",
            "- **STEPS TO REPRODUCE**: \n",
            "1. Type 'AB' into the search bar.\n",
            "2. Observe that no suggested results are shown.\n",
            "- **EXPECTED RESULT**: Suggested results should appear for product codes under three characters.\n",
            "- **ACTUAL RESULT**: No suggested results are shown.\n",
            "- **ENVIRONMENT**: Web version in Safari.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test 4:\n",
            "Input: I tried to reset my password but never got the email. Checked spam, nothing there. I tried it twice, still nothing. My colleague got theirs instantly. Using company email, Outlook on Windows 10.\n",
            "Model output: **SUMMARY:** Password reset email never received.\n",
            "\n",
            "**PRIORITY:** Critical\n",
            "\n",
            "**STEPS TO REPRODUCE:** \n",
            "1. Click on the \"Forgot Password\" link.\n",
            "2. Enter my email address.\n",
            "3. Click \"Send Reset Link.\"\n",
            "4. Check spam folder (no emails).\n",
            "5. Click \"Send Reset Link\" (twice) but no email received.\n",
            "\n",
            "**EXPECTED RESULT:** Receive a password reset email.\n",
            "\n",
            "**ACTUAL RESULT:** No password reset email received.\n",
            "\n",
            "**ENVIRONMENT:**  \n",
            "- User: John Doe  \n",
            "- Device: Outlook on Windows 10  \n",
            "- Company email: john.doe@example.com  \n",
            "- SMTP server: smtp.example.com  \n",
            "- Email client: Outlook  \n",
            "- Operating System: Windows 10  \n",
            "- Browser: Chrome  \n",
            "- Company domain: example.com  \n",
            "- Password reset link verification: Not specified.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Test 5:\n",
            "Input: Not sure if it's a bug, but on the dashboard, the revenue numbers for May are showing as zero, even though we definitely made sales. April and June look fine. This is on the desktop app, version 5.0.0.\n",
            "Model output: ### SUMMARY\n",
            "Revenue numbers for May are showing as zero despite made sales.\n",
            "\n",
            "### PRIORITY\n",
            "Critical\n",
            "\n",
            "### STEPS TO REPRODUCE\n",
            "1. Log into the desktop app.\n",
            "2. Access the dashboard.\n",
            "3. Verify the revenue numbers for May.\n",
            "\n",
            "### EXPECTED RESULT\n",
            "The revenue numbers for May should accurately reflect the sales made in that month.\n",
            "\n",
            "### ACTUAL RESULT\n",
            "The revenue numbers for May show as zero.\n",
            "\n",
            "### ENVIRONMENT\n",
            "Desktop app version 5.0.0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ðŸŽ‰ Testing completed!\n",
            "\n",
            "Your model 'custom-task-model-001' has been trained to: \n",
            "Convert informal bug reports into structured JIRA-style tickets with these exact sections:\n",
            "- SUMMARY: (one line title)\n",
            "- PRIORITY: (Critical/High/Medium/Low based on impact)\n",
            "- STEPS TO REPRODUCE: (numbered list)\n",
            "- EXPECTED RESULT: (what should happen)\n",
            "- ACTUAL RESULT: (what actually happens)\n",
            "- ENVIRONMENT: (extracted system/version info)\n",
            "\n",
            "\n",
            "To use this model in production:\n",
            "1. The model checkpoint is saved in ./.art/\n",
            "2. You can load it using the vLLM library\n",
            "3. Or continue training with more examples by adjusting the configuration at the top\n"
          ]
        }
      ],
      "source": [
        "# Generate test inputs\n",
        "print(\"Generating test inputs...\")\n",
        "test_inputs = await generate_training_inputs(TASK_DESCRIPTION, num_examples=NUM_TEST_INPUTS)\n",
        "\n",
        "print(f\"\\nðŸ§ª Testing the trained model on {len(test_inputs)} new inputs:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, test_input in enumerate(test_inputs):\n",
        "    print(f\"\\nTest {i+1}:\")\n",
        "    print(f\"Input: {test_input}\")\n",
        "\n",
        "    # Run the model\n",
        "    test_task_input = TaskInput(\n",
        "        step=999,\n",
        "        input_text=test_input\n",
        "    )\n",
        "    result_trajectory = await rollout(model, test_task_input)\n",
        "\n",
        "    # Extract the model's response\n",
        "    messages = result_trajectory.messages()\n",
        "    model_response = messages[-1]['content'] if messages else \"No response\"\n",
        "\n",
        "    print(f\"Model output: {model_response}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\nðŸŽ‰ Testing completed!\")\n",
        "print(f\"\\nYour model '{MODEL_NAME}' has been trained to: {TASK_DESCRIPTION}\")\n",
        "print(\"\\nTo use this model in production:\")\n",
        "print(\"1. The model checkpoint is saved in ./.art/\")\n",
        "print(\"2. You can load it using the vLLM library\")\n",
        "print(\"3. Or continue training with more examples by adjusting the configuration at the top\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps\n",
        "\n",
        "Congratulations! You've successfully trained a custom model for your task using only:\n",
        "- A task description\n",
        "- Example inputs (no outputs needed!)\n",
        "- RULER's automatic evaluation\n",
        "\n",
        "Here are some ways to improve results:\n",
        "\n",
        "1. **More diverse inputs**: Generate more varied input examples\n",
        "2. **Longer training**: Increase the number of training steps\n",
        "3. **More comparisons**: Increase `rollouts_per_group` for better RULER comparisons\n",
        "4. **Task refinement**: Make your task description more specific and detailed\n",
        "5. **Hyperparameter tuning**: Adjust learning rate, batch size, etc.\n",
        "\n",
        "Remember: RULER learns what \"good\" means from your task description alone - no labeled data required!\n",
        "\n",
        "For more advanced use cases, check out the [ART documentation](https://art.openpipe.ai)."
      ],
      "metadata": {
        "id": "FuevYgXT-I1h"
      }
    }
  ]
}